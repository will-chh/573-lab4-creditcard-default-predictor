{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab4-edit_this.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Putting it all together in a mini project\n",
    "\n",
    "For this lab, **you can choose to work alone or in a group of up to three students**. You are in charge of how you want to work and who you want to work with. Maybe you really want to go through all the steps of the ML process yourself or maybe you want to practice your collaboration skills, it is up to you! Just remember to indicate who your group members are (if any) when you submit on Gradescope. If you choose to work in a group, you only need to use one GitHub repo (you can create one on github.ubc.ca and set the visibility to \"public\"). If it takes a prohibitively long time to run any of the steps on your laptop, it is OK if you sample the data to reduce the runtime, just make sure you write a note about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Submission instructions\n",
    "rubric={mechanics}\n",
    "\n",
    "<p>You receive marks for submitting your lab correctly, please follow these instructions:</p>\n",
    "\n",
    "<ul>\n",
    "  <li><a href=\"https://ubc-mds.github.io/resources_pages/general_lab_instructions/\">\n",
    "      Follow the general lab instructions.</a></li>\n",
    "  <li><a href=\"https://github.com/UBC-MDS/public/tree/master/rubric\">\n",
    "      Click here to view a description of the rubrics used to grade the questions</a></li>\n",
    "  <li>Make at least three commits.</li>\n",
    "  <li>Push your <code>.ipynb</code> file to your GitHub repository for this lab and upload it to Gradescope.</li>\n",
    "    <ul>\n",
    "      <li>Before submitting, make sure you restart the kernel and rerun all cells.</li>\n",
    "    </ul>\n",
    "  <li>Make sure to only make one gradescope submission per group, and to assign all group members on gradescope at submission time.</li>\n",
    "  <li>Also upload a <code>.pdf</code> export of the notebook to facilitate grading of manual questions (preferably WebPDF, you can select two files when uploading to gradescope)</li>\n",
    "  <li>Don't change any variable names that are given to you, don't move cells around, and don't include any code to install packages in the notebook.</li>\n",
    "  <li>The data you download for this lab <b>SHOULD NOT BE PUSHED TO YOUR REPOSITORY</b> (there is also a <code>.gitignore</code> in the repo to prevent this).</li>\n",
    "  <li>Include a clickable link to your GitHub repo for the lab just below this cell\n",
    "    <ul>\n",
    "      <li>It should look something like this https://github.ubc.ca/MDS-2020-21/DSCI_531_labX_yourcwl.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "https://github.com/will-chh/573-lab4-creditcard-default-predictor#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Introduction <a name=\"in\"></a>\n",
    "\n",
    "In this lab you will be working on an open-ended mini-project, where you will put all the different things you have learned so far in 571 and 573 together to solve an interesting problem.\n",
    "\n",
    "A few notes and tips when you work on this mini-project: \n",
    "\n",
    "#### Tips\n",
    "1. Since this mini-project is open-ended there might be some situations where you'll have to use your own judgment and make your own decisions (as you would be doing when you work as a data scientist). Make sure you explain your decisions whenever necessary. \n",
    "2. **Do not include everything you ever tried in your submission** -- it's fine just to have your final code. That said, your code should be reproducible and well-documented. For example, if you chose your hyperparameters based on some hyperparameter optimization experiment, you should leave in the code for that experiment so that someone else could re-run it and obtain the same hyperparameters, rather than mysteriously just setting the hyperparameters to some (carefully chosen) values in your code. \n",
    "3. If you realize that you are repeating a lot of code try to organize it in functions. Clear presentation of your code, experiments, and results is the key to be successful in this lab. You may use code from lecture notes or previous lab solutions with appropriate attributions. \n",
    "\n",
    "#### Assessment\n",
    "We don't have some secret target score that you need to achieve to get a good grade. **You'll be assessed on demonstration of mastery of course topics, clear presentation, and the quality of your analysis and results.** For example, if you just have a bunch of code and no text or figures, that's not good. If you instead try several reasonable approaches and you have clearly motivated your choices, but still get lower model performance than your friend, don't sweat it.\n",
    "\n",
    "\n",
    "#### A final note\n",
    "Finally, the style of this \"project\" is different from other assignments. It'll be up to you to decide when you're \"done\" -- in fact, this is one of the hardest parts of real projects. But please don't spend WAY too much time on this... perhaps \"several hours\" but not \"many hours\" is a good guideline for a high quality submission. Of course if you're having fun you're welcome to spend as much time as you want! But, if so, try not to do it out of perfectionism or getting the best possible grade. Do it because you're learning and enjoying it. Students from the past cohorts have found such kind of labs useful and fun and we hope you enjoy it as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## 1. Pick your problem and explain the prediction problem <a name=\"1\"></a>\n",
    "rubric={reasoning}\n",
    "\n",
    "In this mini project, you will pick one of the following problems: \n",
    "\n",
    "1. A classification problem of predicting whether a credit card client will default or not. For this problem, you will use [Default of Credit Card Clients Dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset). In this data set, there are 30,000 examples and 24 features, and the goal is to estimate whether a person will default (fail to pay) their credit card bills; this column is labeled \"default.payment.next.month\" in the data. The rest of the columns can be used as features. You may take some ideas and compare your results with [the associated research paper](https://www.sciencedirect.com/science/article/pii/S0957417407006719), which is available through [the UBC library](https://www.library.ubc.ca/). \n",
    "\n",
    "OR \n",
    "\n",
    "2. A regression problem of predicting `reviews_per_month`, as a proxy for the popularity of the listing with [New York City Airbnb listings from 2019 dataset](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data). Airbnb could use this sort of model to predict how popular future listings might be before they are posted, perhaps to help guide hosts create more appealing listings. In reality they might instead use something like vacancy rate or average rating as their target, but we do not have that available here.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Spend some time understanding the problem and what each feature means. Write a few sentences on your initial thoughts on the problem and the dataset. \n",
    "2. Download the dataset and read it as a pandas dataframe. \n",
    "3. Carry out any preliminary preprocessing, if needed (e.g., changing feature names, handling of NaN values etc.)\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "Our problem of interest is predicting whether a credit card client will default on their payment next month. Given a dataset of 30,000 examples and 24 features, we aim to build a binary classification model that can accurately predict default payment behavior. This can be used by financial institutions in areas such as risk assessment and decision-making regarding credit issuance. \n",
    "\n",
    "- ID: row identifier \n",
    "- LIMIT_BAL: amount of given credit in NT dollars (includes individual and family/supplementary credit)\n",
    "- SEX: gender (1=male, 2=female)\n",
    "- EDUCATION: education level (1=graduate school; 2=university; 3=high school; 4=others)\n",
    "- MARRIAGE: marital status (1=married; 2=single; 3=others)\n",
    "- AGE: age in years         \n",
    "- PAY_0 to PAY_6: history of past monthly payment (from September 2005 to April 2005). Values are -1=pay duly (no delay), 1=payment delay for one month, 2=payment delay for two months, and so on.\n",
    "- PAY_AMT1 to PAY_AMT6: amount of previous payment (payment status from the last 6 months starting from September)\n",
    "- BILL_AMT1 to BILL_AMT6: amount of bill statement (from September 2005 to April 2005)\n",
    "- default.payment.next.month: default payment (1=yes, 0=no). This is our target variable\n",
    "\n",
    "There are a mix of categorical and numerical features in the dataset, demographic features include SEX, EDUCATION, MARRIAGE, and AGE. Financial features include LIMIT_BAL, PAY_0 to PAY_6, PAY_AMT1 to PAY_AMT6, and BILL_AMT1 to BILL_AMT6. Behavioural features could include payment history (PAY_0 to PAY_6) and previous payment amounts (PAY_AMT1 to PAY_AMT6). An initial thought is that PAY_0 to PAY_6 may be most predictive of default payment next month, as they likely reflect the client's payment behaviour over the past six months.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data & plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    ")\n",
    "\n",
    "# Preprocessing / transformations\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# Models — classification & regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor \n",
    "\n",
    "# Additional things to import \n",
    "from scipy.stats import loguniform, randint\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Muting Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\"You passed a .* to `is_pandas_dataframe`\",\n",
    "    module=\"altair.utils.data\",\n",
    "    category=UserWarning,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "cr_card_df = pd.read_csv('data_file/UCI_Credit_Card.csv', header=0)\n",
    "#cr_card_df = cr_card_df.sample(frac=0.25, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the first 5 rows of data\n",
    "cr_card_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values\n",
    "cr_card_df.info() #checking datatypes and non-null counts\n",
    "cr_card_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning column names so that they are more consistent and readable (in snake_case)\n",
    "cr_card_df.columns = cr_card_df.columns.str.lower().str.replace('.','_')\n",
    "print(cr_card_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping irrelevant columns\n",
    "cr_card_df = cr_card_df.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#according to the research paper, there are some extra categories that are not represented correctly\n",
    "print(cr_card_df['education'].value_counts().sort_index()) #5,6 are undefined/unknown, can be treated as 'others'\n",
    "print(cr_card_df['marriage'].value_counts().sort_index()) #0 is undefined, can be treated as 'others'\n",
    "\n",
    "cr_card_df['education'] = cr_card_df['education'].replace({5:4, 6:4, 0:4})\n",
    "cr_card_df['marriage'] = cr_card_df['marriage'].replace({0:3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking values after they are grouped to 'others' categories\n",
    "print(cr_card_df['education'].value_counts().sort_index())\n",
    "print(cr_card_df['marriage'].value_counts().sort_index()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking class imbalance \n",
    "cr_card_df['default_payment_next_month'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## 2. Data splitting <a name=\"2\"></a>\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the data into train and test portions.\n",
    "\n",
    "> Make the decision on the `test_size` based on the capacity of your laptop. \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# splitting into 70% train and 30% test portions\n",
    "cr_card_train, cr_card_test = train_test_split(\n",
    "    cr_card_df, \n",
    "    test_size=0.3, \n",
    "    random_state=123\n",
    "    )\n",
    "\n",
    "# Below could be dropped\n",
    "X_train = cr_card_train.drop(columns=['default_payment_next_month'])\n",
    "y_train = cr_card_train['default_payment_next_month']\n",
    "X_test = cr_card_test.drop(columns=['default_payment_next_month'])\n",
    "y_test = cr_card_test['default_payment_next_month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## 3. EDA <a name=\"3\"></a>\n",
    "rubric={viz,reasoning}\n",
    "    \n",
    "Perform exploratory data analysis on the train set.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Include at least two summary statistics and two visualizations that you find useful, and accompany each one with a sentence explaining it.\n",
    "2. Summarize your initial observations about the data. \n",
    "3. Pick appropriate metric/metrics for assessment. \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "2. The profile report shows that there are no missing values in the dataset. The target variable, default_payment_next_month, has a class imbalance with 22.3% of instances being 1 (default) and 77.7% being 0 (no default). This indicates that the model might need to account for this imbalance during training. \\\n",
    "In the correlations section, we can see that pay_0 has the highest positive correlation with the target variable, suggesting that recent payment history is a strong predictor of default payment next month. BILL_AMT1 also shows a moderate positive correlation, indicating that higher bill amounts may be associated with a higher likelihood of default. \\\n",
    "Many financial features like bill_amt, pay_amt are highly skewed to the right, indicating that most clients have lower amounts while a few have very high amounts. This suggests that transformations like log transformation might be beneficial for these features.\n",
    "\n",
    "3. Based on the class imbalance in the target variable, accuracy may not be sufficient to evaluate model performance. Therefore, we will use recall, which indicates how many actual defaults were correctly identified by the model, since missed defaults are the most expensive and important to identify. We can consider changing the decision threshold to optimize recall. Additionally, we will also report precision and F1-score to provide a more comprehensive evaluation of the model's performance (including a confusion matrix), as well as the ROC-AUC score to assess the model's ability to discriminate between classes across different thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "profile = ProfileReport(cr_card_train, title='Credit card default training set EDA', explorative=True)\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cr_card_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Correlation Table for all numerical features against our target column \n",
    "\n",
    "num_cols0 = cr_card_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "corr_w_target = (cr_card_train[num_cols0]\n",
    "                 .corrwith(cr_card_train['default_payment_next_month'].astype(int))\n",
    "                 .sort_values(ascending=False))\n",
    "\n",
    "table = corr_w_target.reset_index()\n",
    "table.columns = (['feature', 'correlation with default'])\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief interpretation on Correlation outcome\n",
    "\n",
    "1. With pay_0 at 0.3251, customers with worse repayment status last month (higher PAY_0) are more likely to default next month.\n",
    "\n",
    "2. With education at 0.0326, age at 0.0107, and sex at -0.0463, these 3 fields do not clearly increase or decrease default risk, therefore, are considered to be dropped. \n",
    "\n",
    "3. With limit_bal at -0.1492, customers with higher credit limit has lower chance of defaulting, which makes sense in real life. \n",
    "\n",
    "4. All bill_amt* fields have small negative values, indicating a higher billed amount are associated with lower default risks. \n",
    "\n",
    "5. All pay_amt* fields have negatives values, with pay_amt1 = -0.7156, it indicates higher and more recent payments are associated with lower default risks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a summary table of numeric features for each class (of our target)\n",
    "num_feats = cr_card_train.select_dtypes(include=['float64']).columns\n",
    "\n",
    "cr_card_train[['default_payment_next_month'] + list(num_feats)].melt(\n",
    "    id_vars='default_payment_next_month',\n",
    "    var_name='feature',\n",
    "    value_name='value'\n",
    ").pivot_table(\n",
    "    index='feature', columns='default_payment_next_month', \n",
    "    values='value',\n",
    "    aggfunc=['mean', 'min', 'max', 'std'], observed=False\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief interpretation on the summary table: \n",
    "\n",
    "1. People who do NOT default pay more and have higher limits.\n",
    "\n",
    "2. Defaulters have: lower credit limits, lower bill amounts, and lower historical payment amounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#given that pay_* are our strongest features, visualizing the distribution of values within each pay_* feature can help us differentiate and understand it \n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "pay_cols = ['pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']\n",
    "\n",
    "pay_long = cr_card_train[pay_cols + ['default_payment_next_month']].melt(\n",
    "    id_vars='default_payment_next_month',\n",
    "    var_name='pay_month',\n",
    "    value_name='delay_status')\n",
    "\n",
    "alt.Chart(pay_long).mark_bar().encode(\n",
    "    x=alt.X('delay_status:O'),\n",
    "    y=alt.Y('count():Q', title='Count of Clients'),\n",
    "    color=alt.Color('default_payment_next_month:N', title='default')\n",
    ").facet('pay_month:N', columns=3).properties(title='Payment Status distribution by month')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feats = ['sex', 'education', 'marriage']\n",
    "\n",
    "cat_data = cr_card_train[categorical_feats + ['default_payment_next_month']].melt(\n",
    "    id_vars='default_payment_next_month',\n",
    "    var_name='feature',\n",
    "    value_name='value')\n",
    "\n",
    "alt.Chart(cat_data).mark_bar().encode(\n",
    "    x=alt.X('value:N'),\n",
    "    y=alt.Y('count():Q', title='Count of Clients'),\n",
    "    color=alt.Color('default_payment_next_month:N', title=\"default\")\n",
    ").facet('feature:N', columns=3).properties(title='Categorical feature distribution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A deeper dive into the correlation between amount of bill statement and repayment status\n",
    "num_feats = cr_card_train.select_dtypes(include=['float64']).columns\n",
    "\n",
    "corr = cr_card_train[num_feats].corr()\n",
    "\n",
    "corr_long = corr.reset_index().melt(\n",
    "    id_vars='index',\n",
    "    var_name='feature',\n",
    "    value_name='correlation')\n",
    "\n",
    "heatmap = alt.Chart(corr_long).mark_rect().encode(\n",
    "    x=alt.X('feature:O', title='feature'),\n",
    "    y=alt.Y('index:O', title='feature'),\n",
    "    color=alt.Color('correlation:Q')\n",
    ") \n",
    "\n",
    "heatmap + heatmap.mark_text().encode(\n",
    "    text=alt.Text('correlation:Q', format='.1f'),\n",
    "    color=alt.value('black')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Interpretation on the Correlation Heat Map \n",
    "\n",
    "Key Info only: \n",
    "1. Bill_amt1 - 6 are highly correlated, leading to multicollinearity issue.\n",
    "2. Pay_amt1 - 6 are somewhat correlated, multicollinearity is still a concern.\n",
    "3. Limit_bal has some correlation with other fields, it could be a very useful feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "# Ensure default is categorical with readable labels\n",
    "cr_card_train['default_label'] = cr_card_train['default_payment_next_month'].map({0: 'No Default', 1: 'Default'})\n",
    "\n",
    "box = (\n",
    "    alt.Chart(cr_card_train)\n",
    "    .mark_boxplot(size=60)\n",
    "    .encode(\n",
    "        x=alt.X('default_label:N', title='Default Status'),\n",
    "        y=alt.Y('limit_bal:Q', title='Credit Limit'),\n",
    "        color=alt.Color('default_label:N', title='Default Status')\n",
    "    )\n",
    "    .properties(\n",
    "        width=300,\n",
    "        height=350,\n",
    "        title='Boxplot of Credit Limit by Default Status'\n",
    "    )\n",
    ")\n",
    "\n",
    "box\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## 4. Feature engineering (Challenging)\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Carry out feature engineering. In other words, extract new features relevant for the problem and work with your new feature set in the following exercises. You may have to go back and forth between feature engineering and preprocessing.\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering Reasoning: \n",
    "\n",
    "The following 3 new features are created as a sniff test: \n",
    "\n",
    "1. payment difference: pay_0 - pay_6 captures the change in payment behaviour from 6months ago to this month. A negative value indicates improvement (since pay_6 was bad and pay_0 was good), while a positive value indicates worsening behaviour, which may be predictive of default risk (behavioural trend over the 6 mo)\n",
    "\n",
    "2. average pay amount: avg_pay_amt = captures the average payment amount over the past 6 months, which may indicate the client's ability to pay the next one\n",
    "\n",
    "3. standard deviation of pay: captures payment volatility over the past 6 months, which may indicate inconsistent payment behaviour (for example: high standard deviation 0,1,3,5,0,6 -> erratic payment behaviour -> higher default risk). Low std (1,1,1,1,1,1) indicates consistent payment behaviour -> lower default risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate features for pay*\n",
    "\n",
    "#Creating copies on the cr_card_train before having new features \n",
    "cr_card_train_fe = cr_card_train.copy()\n",
    "cr_card_test_fe = cr_card_test.copy()\n",
    "\n",
    "#updating this line to have pay_0 as its own feature as pay_0 deviates from the rest of the pay_* features\n",
    "#averaging only from pay_2 to pay_6\n",
    "hist_pay_cols = ['pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']\n",
    "\n",
    "# Train\n",
    "cr_card_train_fe['pay_avg_2_6'] = cr_card_train_fe[hist_pay_cols].mean(axis=1)\n",
    "cr_card_train_fe['pay_past_std'] = cr_card_train_fe[hist_pay_cols].std(axis=1)\n",
    "cr_card_train_fe['pay_diff'] = cr_card_train_fe['pay_0'] - cr_card_train_fe['pay_6']\n",
    "\n",
    "# Test\n",
    "cr_card_test_fe['pay_avg_2_6'] = cr_card_test_fe[hist_pay_cols].mean(axis=1)\n",
    "cr_card_test_fe['pay_past_std'] = cr_card_test_fe[hist_pay_cols].std(axis=1)\n",
    "cr_card_test_fe['pay_diff'] = cr_card_test_fe['pay_0'] - cr_card_test_fe['pay_6']\n",
    "\n",
    "\n",
    "cr_card_train_fe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_cols = ['pay_avg_2_6', 'pay_past_std', 'pay_diff']\n",
    "\n",
    "eng_long = cr_card_train_fe[eng_cols + ['default_payment_next_month']].melt(\n",
    "    id_vars='default_payment_next_month',\n",
    "    var_name='feature',\n",
    "    value_name='value')\n",
    "\n",
    "alt.Chart(eng_long).mark_boxplot(size=40).encode(\n",
    "    x=alt.X('default_payment_next_month:O'),\n",
    "    y=alt.Y('value:Q'),\n",
    "    color=alt.Color('default_payment_next_month:N'), \n",
    "    facet=alt.Facet('feature:N', columns=3)\n",
    ").properties(width=180, height=240, title='Distribution of engineered payment features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_long = cr_card_train_fe[eng_cols + ['default_payment_next_month']].melt(\n",
    "    id_vars='default_payment_next_month',\n",
    "    var_name='feature',\n",
    "    value_name='value')\n",
    "\n",
    "alt.Chart(eng_long).mark_bar(opacity=0.8).encode(\n",
    "    x=alt.X('value:Q', bin=alt.Bin(maxbins=30)),\n",
    "    y=alt.Y('count():Q'),\n",
    "    color=alt.Color('default_payment_next_month:N'), \n",
    "    facet=alt.Facet('feature:N', columns=3)\n",
    ").properties(width=180, height=240, title='Distribution of engineered payment features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## 5. Preprocessing and transformations <a name=\"5\"></a>\n",
    "rubric={accuracy,reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Identify different feature types and the transformations you would apply on each feature type. \n",
    "2. Define a column transformer, if necessary. \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Set up\n",
    "In Q4, it explicitly mentioned to work with the newly created features in the following exercises, therefore we will be using using the transformed data sets for the rest of the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_card_test_fe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#Creating Copies here, all transformations will be working on datasets ending without *_fe\n",
    "cr_card_train = cr_card_train_fe.copy()\n",
    "cr_card_test = cr_card_test_fe.copy()\n",
    "\n",
    "\n",
    "X_train = cr_card_train.drop(columns=['default_payment_next_month'])\n",
    "y_train = cr_card_train['default_payment_next_month']\n",
    "\n",
    "X_test = cr_card_test.drop(columns=['default_payment_next_month'])\n",
    "y_test = cr_card_test['default_payment_next_month']\n",
    "\n",
    "\n",
    "#Grouping feature types \n",
    "numeric_feats = ['limit_bal', \n",
    "                 'bill_amt1', 'bill_amt2', 'bill_amt3', 'bill_amt4', 'bill_amt5', 'bill_amt6',\n",
    "                 'pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6',\n",
    "                 'pay_avg_2_6', 'pay_past_std', 'pay_diff', # engineered\n",
    "                 'age']\n",
    "\n",
    "ordinal_feats = ['education', 'pay_0'] \n",
    "\n",
    "categorical_feats = ['sex', 'marriage']\n",
    "\n",
    "drop_feats = ['pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=drop_feats)\n",
    "X_test = X_test.drop(columns=drop_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying transformations \n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), numeric_feats),\n",
    "    (OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), ordinal_feats),\n",
    "    (OneHotEncoder(handle_unknown='ignore'), categorical_feats),\n",
    "    #(\"drop\", drop_feats)\n",
    ")\n",
    "\n",
    "#Fitting preprocessor on X_train only \n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "#Transforming both data sets \n",
    "X_train_enc = preprocessor.transform(X_train)\n",
    "X_test_enc = preprocessor.transform(X_test)\n",
    "\n",
    "#Collecting column names, ordering matters, pls refer above, numeric to ordinal to onehot\n",
    "all_columns = preprocessor.get_feature_names_out()\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_enc, columns=all_columns, index=X_train.index)\n",
    "X_test_df  = pd.DataFrame(X_test_enc, columns=all_columns, index=X_test.index)\n",
    "\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## 6. Baseline model <a name=\"6\"></a>\n",
    "rubric={accuracy}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Train a baseline model for your task and report its performance.\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "cross_val_results = {}\n",
    "\n",
    "dummy = DummyClassifier(strategy = \"most_frequent\")\n",
    "scoring_metrics = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']\n",
    "\n",
    "cross_val_results['dummy'] = (pd.DataFrame(\n",
    "    cross_validate(\n",
    "        dummy, \n",
    "        X_train_enc, \n",
    "        y_train, \n",
    "        scoring = scoring_metrics, \n",
    "        cv=5, \n",
    "        return_train_score=True))\n",
    "    .agg(['mean', 'std'])\n",
    "    .round(3)\n",
    "    .T)\n",
    "\n",
    "cross_val_results['dummy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Interpretation\n",
    "\n",
    "Accuracy score of 0.777 confirms the strong class imbalance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## 7. Linear models <a name=\"7\"></a>\n",
    "rubric={accuracy,reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try a linear model as a first real attempt. \n",
    "2. Carry out hyperparameter tuning to explore different values for the regularization hyperparameter. \n",
    "3. Report cross-validation scores along with standard deviation. \n",
    "4. Summarize your results.\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Models Results Summary - Updated\n",
    "\n",
    "A Logistic Regression model with class_weight='balanced' was used as the first linear model for predicting credit card defaults. Hyperparameter tuning was performed using RandomizedSearchCV to optimize the regularization strength, and the best value of C was approximately 0.015. This relatively small value indicates that stronger regularization helped stabilize the model and prevent overfitting. The tuned model achieved a cross-validated test accuracy of about 0.721 with a low standard deviation across folds, suggesting that performance was stable and not sensitive to the specific subset of training data used. More importantly, the tuned model achieved an F1 score of approximately 0.506, with precision around 0.418 and recall around 0.642. Because recall is especially important in identifying customers who will default, the tuned model’s ability to capture roughly 64% of the actual defaulters represents a substantial improvement over the Dummy Classifier baseline. The ROC AUC of 0.735 further indicates that the model has reasonable discriminative ability between default and non-default cases.\n",
    "\n",
    "Comparing the tuned model to the untuned version reveals relatively small but meaningful improvements. The untuned model, which uses the default C = 1.0, produced a slightly weaker cross-validation score and showed marginally poorer calibration when predicting the positive class. The confusion matrices illustrate this difference clearly: the tuned model reduced the number of false positives from 1,871 to 1,860 while increasing the number of true negatives by the same amount. True positives and false negatives remained unchanged across both models, meaning the tuned regularization primarily adjusted the model’s confidence on borderline negative-class predictions. Although these changes appear small, they are consistent with the tuning objective and reflect the limited flexibility of a linear model on this dataset. Overall, the tuned logistic regression model provides stable performance, improves recall compared to the untuned version, and establishes a reasonable, well-regularized linear baseline for subsequent comparison with more flexible nonlinear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#Model 1 - Logistics Regression\n",
    "\n",
    "pipe_lr = make_pipeline(\n",
    "    preprocessor, \n",
    "    LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    )\n",
    "\n",
    "# Hyperparameter random search space for C (inverse regularization strength), on a log scale\n",
    "param_dist = {\"logisticregression__C\": loguniform(0.01, 10)}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe_lr, \n",
    "    param_dist, \n",
    "    n_iter=20, \n",
    "    cv=5, \n",
    "    n_jobs=-1,\n",
    "    random_state=123, \n",
    "    return_train_score=True\n",
    "    )\n",
    "\n",
    "# Carrying out random search, raw X_train is fine\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best pipeline (preprocessor + tuned LogisticRegression)\n",
    "pipe_lr_tuned = random_search.best_estimator_\n",
    "\n",
    "logreg_cv_scores = pd.DataFrame(cross_validate(\n",
    "    pipe_lr_tuned, \n",
    "    X_train, y_train, \n",
    "    cv=5, \n",
    "    scoring = scoring_metrics, \n",
    "    return_train_score=True\n",
    ")).agg(['mean', 'std']).round(3).T\n",
    "\n",
    "cross_val_results['logreg'] = logreg_cv_scores\n",
    "\n",
    "cross_val_results['logreg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"Best Parameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"CV Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Untuned Logistics Regression Pipeline\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "predictions = pipe_lr.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned Logistics Regression Pipeline\n",
    "pipe_lr_tuned.fit(X_train, y_train)\n",
    "\n",
    "predictions = pipe_lr_tuned.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "# Plot confusion matrix on the TUNED Model, there is barely any improvement. \n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe_lr_tuned,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=[\"No Default\", \"Default\"],\n",
    "    cmap=\"Blues\",\n",
    "    values_format=\"d\",\n",
    ")\n",
    "\n",
    "disp.ax_.set_title(\"Logistic Regression – Confusion Matrix (Test Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## 8. Different models <a name=\"8\"></a>\n",
    "rubric={accuracy,reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Try out three other models aside from the linear model. \n",
    "2. Summarize your results in terms of overfitting/underfitting and fit and score times. Can you beat the performance of the linear model? \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "#### Interpretation of Non-Linear Models (SVC, Gradient Boosting, Random Forest)\n",
    "\n",
    "To compare non-linear models against the tuned logistic regression baseline, three additional classifiers were evaluated: an SVC with an RBF kernel, a Gradient Boosting Classifier, and a Random Forest with class balancing. All models were evaluated using identical 5-fold cross-validation and the same preprocessing pipeline, allowing for a fair comparison across accuracy, F1, precision, recall, ROC AUC, and fit/score times.\n",
    "\n",
    "Across the board, the non-linear models achieved higher accuracy and ROC AUC than logistic regression. Gradient Boosting obtained the highest test accuracy (≈0.821), with Random Forest close behind (≈0.815). SVC also outperformed logistic regression in both accuracy (0.767 vs. 0.721) and ROC AUC (0.759 vs. 0.735), confirming that non-linear decision boundaries better fit the complex structure of the data than a linear model. However, when focusing on metrics relevant to the minority (default) class—particularly recall and F1—the story becomes more nuanced. Logistic regression continues to outperform the non-linear models in recall (0.642 for LR vs. 0.607 for SVC, 0.377 for GBC, and 0.352 for RF). Even though the tree-based models achieve higher precision (≈0.66–0.71), they identify substantially fewer actual defaults, resulting in lower F1 scores. This reflects a fundamental tradeoff: boosting and random forests tend to be conservative in predicting the minority class unless explicitly tuned for recall.\n",
    "\n",
    "From an overfitting perspective, the Random Forest shows signs of severe overfitting: training accuracy, F1, precision, recall, and ROC AUC are all effectively 1.000, while test scores drop sharply (e.g., recall falling to 0.352). This indicates that the forest memorizes the training data but struggles to generalize to unseen examples. Gradient Boosting shows milder overfitting, with training accuracy (0.829) slightly exceeding test accuracy (0.821), and similarly small but consistent gaps across other metrics. SVC falls in between—its gap between train and test scores is modest but noticeable, consistent with the flexible non-linear decision boundary of the RBF kernel. In contrast, logistic regression shows almost no gap between train and test performance, indicating a very stable and well-regularized model.\n",
    "\n",
    "Fit and score time differences also help explain model behavior. SVC is by far the slowest model, with average fit times near 11 seconds per fold and score times around 7 seconds, reflecting the computational cost of RBF kernel distance calculations on roughly 30,000 samples. Gradient Boosting is faster (≈5.3 s fit), while Random Forest is significantly faster still (≈2.9 s fit). Logistic regression remains extremely efficient at 0.038 seconds, making it attractive when computational resources or latency matter.\n",
    "\n",
    "One surprising observation is that the tree-based models—particularly Gradient Boosting and Random Forest—have much worse recall than logistic regression, even though they often outperform LR on accuracy and ROC AUC. This occurs because tree models trained with default hyperparameters tend to prioritize splits that maximize accuracy rather than detect minority-class instances. Even with class_weight='balanced', Random Forest tends to make conservative positive predictions, resulting in high precision but low recall. Logistic regression, by contrast, distributes probability mass more evenly between classes and, combined with class balancing, becomes better at detecting default cases. With hyperparameter tuning (e.g., adjusting decision thresholds, increasing the number of boosted trees, reducing RF depth), tree models can be pushed toward better recall, but with default settings they tend to underpredict the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Non-linear models outperform logistic regression in accuracy and ROC AUC, but logistic regression remains superior on the recall of the default class, which is the most important business metric in this domain. Random Forest overfits heavily, Gradient Boosting overfits mildly, and SVC shows moderate overfitting but reasonably strong balanced performance. Logistic Regression remains the most stable and interpretable model, even if not the most accurate, demonstrating why model evaluation must go beyond accuracy alone when dealing with imbalanced classification problems such as credit default prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# First Model: SVC Classifier\n",
    "\n",
    "pipe_svc = make_pipeline(\n",
    "    preprocessor, \n",
    "    SVC(class_weight='balanced')\n",
    "    )\n",
    "\n",
    "cross_val_results['SVC'] = pd.DataFrame(cross_validate(\n",
    "    pipe_svc, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    return_train_score=True, \n",
    "    scoring = scoring_metrics, \n",
    "    cv = 5\n",
    "    )).agg(['mean', 'std']).round(3).T\n",
    "\n",
    "cross_val_results['SVC'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Second Model: Gradient Boosting Classifier \n",
    "\n",
    "pipe_GBC = make_pipeline(\n",
    "    preprocessor, \n",
    "    GradientBoostingClassifier(random_state=123)\n",
    "    )\n",
    "\n",
    "cross_val_results['GBC'] = pd.DataFrame(cross_validate(\n",
    "    pipe_GBC, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    return_train_score=True, \n",
    "    scoring = scoring_metrics, \n",
    "    cv = 5\n",
    "    )).agg(['mean', 'std']).round(3).T\n",
    "\n",
    "cross_val_results['GBC'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Model: Random Forest Classifier\n",
    "pipe_tree = make_pipeline(\n",
    "    preprocessor, \n",
    "    RandomForestClassifier(class_weight=\"balanced\", random_state=123)\n",
    "    )\n",
    "\n",
    "cross_val_results['RF'] = pd.DataFrame(cross_validate(\n",
    "    pipe_tree, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    return_train_score=True, \n",
    "    scoring = scoring_metrics, \n",
    "    cv = 5\n",
    "    )).agg(['mean', 'std']).round(3).T\n",
    "\n",
    "cross_val_results['RF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_results_df = pd.concat(cross_val_results, axis=1)\n",
    "cross_val_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## 9. Feature selection (Challenging)\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Make some attempts to select relevant features. You may try `RFECV`, forward/backward selection or L1 regularization for this. Do the results improve with feature selection? Summarize your results. If you see improvements in the results, keep feature selection in your pipeline. If not, you may abandon it in the next exercises unless you think there are other benefits with using fewer features.\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## 10. Hyperparameter optimization\n",
    "rubric={accuracy,reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Make some attempts to optimize hyperparameters for the models you've tried and summarize your results. In at least one case you should be optimizing multiple hyperparameters for a single model. You may use `sklearn`'s methods for hyperparameter optimization or fancier Bayesian optimization methods.  Briefly summarize your results.\n",
    "  - [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)   \n",
    "  - [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "  - [scikit-optimize](https://github.com/scikit-optimize/scikit-optimize) \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "#### Summary of Tuned Model Results\n",
    "\n",
    "Tuning the three non-linear models led to modest but meaningful shifts in performance, with the impact varying by model family. SVC tuning primarily adjusted the regularization (C) and kernel smoothness (gamma), but the overall performance remained almost identical to the untuned version. Test accuracy and ROC AUC stayed essentially unchanged, and the recall improved only slightly (0.607 → 0.609). This reflects the fact that the baseline SVC model was already close to its optimal operating point, and the limited gains came at the cost of dramatically increased fit time (from ~11 seconds to ~58 seconds per fold).\n",
    "\n",
    "For Gradient Boosting, tuning learning rate and depth similarly produced small refinements rather than dramatic improvements. Test accuracy remained 0.821 before and after tuning, and ROC AUC increased only marginally (0.780 → 0.781). Precision remained high (~0.68), but recall continued to lag the linear model at about 0.38. These results suggest that the default hyperparameters of gradient boosting were already well-calibrated to the dataset, and recall performance is limited more by the model’s conservative bias toward predicting the minority class than by its depth or learning rate.\n",
    "\n",
    "Random Forest tuning produced the most noticeable changes. The tuned model substantially reduced overfitting: train accuracy dropped from an unrealistic 0.999 to a more reasonable 0.811, and train ROC AUC fell from 1.000 to 0.859. This indicates that tuning depth and leaf size successfully constrained the model. As a result, test performance improved across all key metrics. Recall increased from 0.352 to 0.593, precision improved slightly, and ROC AUC rose from 0.761 to 0.782, now matching or slightly surpassing Gradient Boosting. Although accuracy decreased slightly (0.815 → 0.784), the tuned model exhibits far more balanced generalization and a significantly stronger ability to identify defaulting customers.\n",
    "\n",
    "Overall, tuning did not fundamentally reorder the models’ performance rankings, but it did produce important refinements—especially for Random Forest, where tuning meaningfully reduced overfitting and achieved substantial improvements in recall and ROC AUC. These tuned models provide a more reliable foundation for interpretation and test-set evaluation in later questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#SVC tune C and gamma\n",
    "\n",
    "# Pipeline: preprocess → SVC (RBF)\n",
    "pipe_svc = make_pipeline(\n",
    "    preprocessor,\n",
    "    SVC(class_weight='balanced', probability=True)  # probability=True needed for ROC AUC\n",
    ")\n",
    "\n",
    "# Search space for SVC\n",
    "# C: controls margin softness (bigger C -> more complex model)\n",
    "# gamma: controls RBF kernel width (bigger gamma -> more wiggly boundary)\n",
    "param_dist_svc = {\n",
    "    \"svc__C\": loguniform(1e-3, 1e2),\n",
    "    \"svc__gamma\": loguniform(1e-4, 1e0),\n",
    "}\n",
    "\n",
    "# Randomized search over SVC hyperparameters\n",
    "svc_search = RandomizedSearchCV(\n",
    "    pipe_svc,\n",
    "    param_distributions=param_dist_svc,\n",
    "    n_iter=20,             # number of random combinations\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",     # optimize for ROC AUC\n",
    "    n_jobs=-1,\n",
    "    random_state=123,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "svc_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best SVC params:\", svc_search.best_params_)\n",
    "print(\"Best SVC CV ROC AUC:\", svc_search.best_score_)\n",
    "\n",
    "pipe_svc_tuned = svc_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#GBC tune learning_rate, max_depth, n_estimators\n",
    "\n",
    "# Pipeline: preprocess → GradientBoosting\n",
    "pipe_gbc = make_pipeline(\n",
    "    preprocessor,\n",
    "    GradientBoostingClassifier(random_state=123)\n",
    ")\n",
    "\n",
    "# Small grid search (tree depth, number of trees, learning rate)\n",
    "param_grid_gbc = {\n",
    "    \"gradientboostingclassifier__learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"gradientboostingclassifier__max_depth\": [2, 3, 4],\n",
    "    \"gradientboostingclassifier__n_estimators\": [100, 200],\n",
    "}\n",
    "\n",
    "gbc_search = GridSearchCV(\n",
    "    pipe_gbc,\n",
    "    param_grid=param_grid_gbc,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "gbc_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best GBC params:\", gbc_search.best_params_)\n",
    "print(\"Best GBC CV ROC AUC:\", gbc_search.best_score_)\n",
    "\n",
    "pipe_gbc_tuned = gbc_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#RF tune n_estimators, max_depth, min_samples_leaf\n",
    "\n",
    "# Pipeline: preprocess → RandomForest\n",
    "pipe_rf = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestClassifier(class_weight=\"balanced\", random_state=123)\n",
    ")\n",
    "\n",
    "# RF search space\n",
    "# n_estimators: number of trees\n",
    "# max_depth: limit depth to reduce overfitting\n",
    "# min_samples_leaf: make leaves less pure, improves generalization\n",
    "param_dist_rf = {\n",
    "    \"randomforestclassifier__n_estimators\": randint(200, 800),\n",
    "    \"randomforestclassifier__max_depth\": [None, 5, 10, 20],\n",
    "    \"randomforestclassifier__min_samples_leaf\": randint(1, 20),\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    pipe_rf,\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    random_state=123,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best RF params:\", rf_search.best_params_)\n",
    "print(\"Best RF CV ROC AUC:\", rf_search.best_score_)\n",
    "\n",
    "pipe_rf_tuned = rf_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Adding SVC tuned score \n",
    "\n",
    "svc_cv_scores = (\n",
    "    pd.DataFrame(\n",
    "        cross_validate(\n",
    "            pipe_svc_tuned,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=5,\n",
    "            scoring=scoring_metrics,\n",
    "            return_train_score=True\n",
    "        )\n",
    "    )\n",
    "    .agg(['mean', 'std'])\n",
    "    .round(3)\n",
    "    .T\n",
    ")\n",
    "\n",
    "cross_val_results['svc_tuned'] = svc_cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Adding GBC tuned score \n",
    "\n",
    "gbc_cv_scores = (\n",
    "    pd.DataFrame(\n",
    "        cross_validate(\n",
    "            pipe_gbc_tuned,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=5,\n",
    "            scoring=scoring_metrics,\n",
    "            return_train_score=True\n",
    "        )\n",
    "    )\n",
    "    .agg(['mean', 'std'])\n",
    "    .round(3)\n",
    "    .T\n",
    ")\n",
    "\n",
    "cross_val_results['gbc_tuned'] = gbc_cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Adding RF tuned score\n",
    "\n",
    "rf_cv_scores = (\n",
    "    pd.DataFrame(\n",
    "        cross_validate(\n",
    "            pipe_rf_tuned,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=5,\n",
    "            scoring=scoring_metrics,\n",
    "            return_train_score=True\n",
    "        )\n",
    "    )\n",
    "    .agg(['mean', 'std'])\n",
    "    .round(3)\n",
    "    .T\n",
    ")\n",
    "\n",
    "cross_val_results['rf_tuned'] = rf_cv_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "cross_val_results_df = pd.concat(cross_val_results, axis=1)\n",
    "cross_val_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## 11. Interpretation and feature importances <a name=\"1\"></a>\n",
    "rubric={accuracy,reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Use the methods we saw in class (e.g., `permutation_importance` or `shap`) (or any other methods of your choice) to examine the most important features of one of the non-linear models. \n",
    "2. Summarize your observations. \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why ROC AUC is best for comparing your non-linear models\n",
    "\n",
    "It does not depend on the probability threshold\n",
    "\n",
    "It evaluates ranking ability, not class calibration\n",
    "\n",
    "It handles imbalanced datasets gracefully\n",
    "\n",
    "It is robust across different model families\n",
    "\n",
    "It reflects general predictive power better than recall/precision alone\n",
    "\n",
    "It is the metric emphasized in your lecture notes for fair model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "#### Permutation Importance on the tuned Random Forest\n",
    "\n",
    "Interpretation\n",
    "\n",
    "The permutation importance results show that recent repayment behavior is by far the strongest signal for predicting credit default. PAY_0, the most recent delinquency indicator, is the top feature by a large margin, confirming that short-term repayment status is the most influential driver of model predictions. The engineered feature pay_avg_2_6, which summarizes repayment behavior across the previous five months, is the second-most important feature, indicating that longer-term repayment patterns also contribute substantial predictive power.\n",
    "\n",
    "Credit limit (limit_bal) ranks next, suggesting that customers with lower credit limits may be more prone to default. Several payment amount variables (pay_amt1, pay_amt2, pay_amt3, etc.) and bill amounts (bill_amt1–bill_amt4, bill_amt6) appear prominently in the top 15, reflecting that both repayment capacity and outstanding balances influence default risk. The engineered volatility feature pay_diff and the standard deviation summary pay_past_std also carry meaningful importance, reinforcing that your feature engineering successfully captured useful behavioral trends.\n",
    "\n",
    "Overall, the model relies primarily on recent and historical repayment behavior, secondarily on balance and payment amount patterns, and far less on demographic variables — a pattern consistent with typical credit-risk models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "rf_tuned = pipe_rf_tuned  # tuned RF pipeline\n",
    "\n",
    "#Permutation importance on the pipeline\n",
    "result = permutation_importance(\n",
    "    rf_tuned,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    n_repeats=10,\n",
    "    scoring='roc_auc',\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "#Use original train column names\n",
    "importances_mean = result.importances_mean\n",
    "perm_sorted_idx = importances_mean.argsort()\n",
    "\n",
    "perm_df = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": X_train.columns,\n",
    "        \"importance_mean\": importances_mean\n",
    "    }\n",
    ").sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "perm_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Plotting out bar chart of the top features \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(\n",
    "    result.importances[perm_sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=X_train.columns[perm_sorted_idx],\n",
    ")\n",
    "plt.xlabel(\"Permutation feature importance (Δ ROC AUC)\")\n",
    "plt.title(\"Tuned Random Forest – Permutation Importances (Train)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## 12. Results on the test set <a name=\"12\"></a>\n",
    "rubric={accuracy,reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try your best performing model on the test data and report test scores. \n",
    "2. Do the test scores agree with the validation scores from before? To what extent do you trust your results? Do you think you've had issues with optimization bias? \n",
    "3. Take one or two test predictions and explain them with SHAP force plots.  \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "#### Interpretation of Test Scores\n",
    "\n",
    "The test-set results for the tuned Random Forest model closely match the validation scores obtained during cross-validation. Earlier, the model achieved a validation ROC AUC of approximately 0.782, and on the test set it reached 0.781, showing almost no degradation in performance. Accuracy (0.776), recall (0.593), and F1 score (0.534) are also consistent with the cross-validation metrics. This alignment suggests that the model generalizes well to unseen data and that we are not suffering from meaningful overfitting.\n",
    "\n",
    "Because the validation and test scores are so similar, I have high confidence in these results. The hyperparameter tuning was performed using a separate training-only cross-validation loop, and the test set was only used once at the end, so the risk of optimization bias is low. The consistency between validation and test ROC AUC—our most reliable metric for imbalanced problems—reinforces that the tuning process produced a stable model rather than one overfitted to the training folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of the SHAP values\n",
    "\n",
    "To understand why the model makes its predictions, I examined SHAP contributions for individual test-set customers. For a specific default case, the SHAP values indicate which features push the model toward predicting default (positive values) and which features push it toward predicting non-default (negative values).\n",
    "\n",
    "For the example below, the model predicted default, and the SHAP values show that the strongest positive contributor was PAY_0, with a SHAP value of +0.194, meaning the customer’s most recent repayment status strongly increased the model’s belief that this person would default. Additional positive contributions came from pay_diff (difference between recent and past repayment severity) and pay_past_std, both suggesting inconsistent or deteriorating payment behavior.\n",
    "\n",
    "In contrast, several features pulled the prediction downward (toward non-default). Higher credit limit, along with relatively stronger historical payments such as pay_avg_2_6, pay_amt3, and multiple bill amounts, all had negative SHAP values. These characteristics suggested financial stability, partially offsetting the risk signals but not enough to override the strong effect from PAY_0 and related repayment-history variables.\n",
    "\n",
    "Overall, the SHAP force and waterfall plots reveal that the model’s decisions are driven primarily by repayment behavior—especially the most recent month—while monetary features such as limit and payment amounts provide secondary stabilizing effects. This aligns with the earlier permutation-importance results and helps confirm that the model is behaving in a financially intuitive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred = pipe_rf_tuned.predict(X_test)\n",
    "y_proba = pipe_rf_tuned.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute test metrics\n",
    "test_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"precision\": precision_score(y_test, y_pred),\n",
    "    \"recall\": recall_score(y_test, y_pred),\n",
    "    \"f1_score\": f1_score(y_test, y_pred),\n",
    "    \"roc_auc\": roc_auc_score(y_test, y_proba)\n",
    "}\n",
    "\n",
    "pd.DataFrame(test_metrics, index=[\"RandomForest_Tuned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#Plotting the confusion matrix here\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe_rf_tuned,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=[\"No Default\", \"Default\"],\n",
    "    cmap=\"Blues\",\n",
    "    values_format=\"d\"\n",
    ")\n",
    "\n",
    "disp.ax_.set_title(\"Confusion Matrix — Tuned Random Forest (Test Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP Force Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Get preprocessor and RF model from the tuned pipeline\n",
    "ct = pipe_rf_tuned.named_steps[\"columntransformer\"]\n",
    "rf_model = pipe_rf_tuned.named_steps[\"randomforestclassifier\"]\n",
    "\n",
    "# Encode the test data (like X_test_enc in lecture)\n",
    "X_test_enc = ct.transform(X_test)\n",
    "feature_names = ct.get_feature_names_out()\n",
    "\n",
    "# Reset y_test index to align with X_test_enc row indices\n",
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "\n",
    "y_test_reset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for each class\n",
    "no_default_idx = y_test_reset[y_test_reset == 0].index.tolist()\n",
    "default_idx    = y_test_reset[y_test_reset == 1].index.tolist()\n",
    "\n",
    "# Pick one example of each (10th in each list just like lecture)\n",
    "ex_no_default_index = no_default_idx[10]\n",
    "ex_default_index    = default_idx[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# TreeExplainer on the RF model\n",
    "rf_explainer = shap.TreeExplainer(rf_model)\n",
    "\n",
    "# Explanation object for all test rows (in encoded space)\n",
    "rf_explanation = rf_explainer(X_test_enc)   # shape: (n_samples, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values for one example (e.g., a default case)\n",
    "ex_idx = ex_default_index  # or ex_no_default_index\n",
    "\n",
    "shap_vals_ex = rf_explanation[ex_idx].values[:, 1]\n",
    "\n",
    "pd.DataFrame(\n",
    "    shap_vals_ex,\n",
    "    index=feature_names,\n",
    "    columns=[\"SHAP values\"],\n",
    ").sort_values(\"SHAP values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_idx = ex_default_index \n",
    "\n",
    "# Base value for this example & class 1 (default)\n",
    "base_val = rf_explanation.base_values[ex_idx, 1]\n",
    "\n",
    "# SHAP values for this example & class 1\n",
    "shap_vals = rf_explanation.values[ex_idx, :, 1]\n",
    "\n",
    "# Force plot (no features argument needed)\n",
    "shap.plots.force(base_val, shap_vals, matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## 13. Summary of results <a name=\"13\"></a>\n",
    "rubric={reasoning}\n",
    "\n",
    "Imagine that you want to present the summary of these results to your boss and co-workers. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a table summarizing important results. \n",
    "2. Write concluding remarks.\n",
    "3. Discuss other ideas that you did not try but could potentially improve the performance/interpretability . \n",
    "3. Report your final test score along with the metric you used at the top of this notebook.\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "#### Concluding Remarks\n",
    "\n",
    "Across all experiments, the tuned Random Forest achieved the strongest balance of predictive power and generalization. Its test ROC AUC of 0.781 closely matches its validation performance, indicating reliable generalization and low risk of overfitting. Recall (0.593) and F1 (0.534) demonstrate that the model captures a meaningful proportion of true defaults while maintaining reasonable precision. SHAP and permutation-importance analyses both highlight repayment behavior—especially PAY_0 and recent payment patterns—as the dominant predictors of default risk, which is consistent with financial intuition.\n",
    "\n",
    "Overall, the model provides actionable predictive performance while maintaining transparency into the main risk drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Improvements\n",
    "\n",
    "Several enhancements could further improve model performance or interpretability:\n",
    "\n",
    "1. Threshold tuning: Adjusting the classification threshold could improve recall or precision depending on business requirements (e.g., reducing missed defaults).\n",
    "\n",
    "2. Cost-sensitive learning: Incorporating asymmetric costs of false negatives vs false positives may better align predictions with real financial risk.\n",
    "\n",
    "3. Feature interactions: Tree-based models implicitly capture interactions, but explicitly engineering domain-specific interactions could strengthen signal.\n",
    "\n",
    "4. Alternative models: XGBoost or LightGBM often outperform Random Forests on tabular data and support built-in handling of class imbalance.\n",
    "\n",
    "5. More granular temporal features: Breaking down repayment history into slope/trend features may capture behavioral changes more effectively.\n",
    "\n",
    "These directions could be explored in future iterations if higher recall or tighter risk ranking is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only test metrics from your cross_val_results_df\n",
    "metrics_to_keep = [\n",
    "    \"test_accuracy\", \n",
    "    \"test_precision\", \n",
    "    \"test_recall\", \n",
    "    \"test_f1\", \n",
    "    \"test_roc_auc\"\n",
    "]\n",
    "\n",
    "summary_table = cross_val_results_df.loc[metrics_to_keep].xs(\"mean\", level=1, axis=1)\n",
    "\n",
    "# Pretty formatting\n",
    "summary_table = summary_table.T.round(3)\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    \"accuracy\": 0.776111,\n",
    "    \"precision\": 0.485907,\n",
    "    \"recall\": 0.592916,\n",
    "    \"f1_score\": 0.534104,\n",
    "    \"roc_auc\": 0.781173\n",
    "}\n",
    "\n",
    "final_results_df = pd.DataFrame(final_results, index=[\"RandomForest_Tuned\"]).T\n",
    "final_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## 14. Creating a data analysis pipeline (Challenging)\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Convert this notebook into scripts to create a reproducible data analysis pipeline with appropriate documentation. Submit your project folder in addition to this notebook on GitHub and briefly comment on your organization in the text box below.\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## 15. Your takeaway from the course (Challenging)\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "What is your biggest takeaway from this course? \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "_Points:_ 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "The biggest takeaway from this course is that there are various ways to improve model performance. One key aspect is selecting the appropriate evaluation metrics, recognizing that accuracy is not always the best measure, and that metrics like precision and recall can provide more context and meaningful insights depending on the problem. Another important approach is feature engineering, which plays a critical role in building effective models and often requires creativity from the data scientist to design new features that can meaningfully improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-danger\" style=\"color:black\">\n",
    "    \n",
    "**Restart, run all and export a PDF before submitting**\n",
    "    \n",
    "Before submitting,\n",
    "don't forget to run all cells in your notebook\n",
    "to make sure there are no errors\n",
    "and so that the TAs can see your plots on Gradescope.\n",
    "You can do this by clicking the ▶▶ button\n",
    "or going to `Kernel -> Restart Kernel and Run All Cells...` in the menu.\n",
    "This is not only important for MDS,\n",
    "but a good habit you should get into before ever committing a notebook to GitHub,\n",
    "so that your collaborators can run it from top to bottom\n",
    "without issues.\n",
    "    \n",
    "After running all the cells,\n",
    "export a PDF of the notebook (preferably the WebPDF export)\n",
    "and upload this PDF together with the ipynb file to Gradescope\n",
    "(you can select two files when uploading to Gradescope)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "feedback"
    ]
   },
   "source": [
    "---\n",
    "\n",
    "## Help us improve the labs\n",
    "\n",
    "The MDS program is continually looking to improve our courses, including lab questions and content. The following optional questions will not affect your grade in any way nor will they be used for anything other than program improvement:\n",
    "\n",
    "1. Approximately how many hours did you spend working or thinking about this assignment (including lab time)?\n",
    "\n",
    "#Ans:\n",
    "\n",
    "2. Do you have any feedback on the lab you be willing to share? For example, any part or question that you particularly liked or disliked?\n",
    "\n",
    "#Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci-573",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
